{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu các URL vào file buy_urls.txt\n"
     ]
    }
   ],
   "source": [
    "# crawl link - buying\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Khởi tạo một danh sách để lưu trữ các URL\n",
    "all_urls = []\n",
    "\n",
    "# Đặt User-Agent để giả mạo trình duyệt\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Sử dụng vòng lặp để duyệt qua các trang\n",
    "for i in range(1, 25):\n",
    "    url = f'https://meeyland.com/mua-ban-nha-dat?page={i}'\n",
    "    \n",
    "    # Gửi yêu cầu GET đến trang web\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Kiểm tra trạng thái phản hồi\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Unable to access page {i}\")\n",
    "        continue\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm tất cả các thẻ div có class là \"relative slider-hover\"\n",
    "    ads = soup.find_all('div', class_='relative slider-hover')\n",
    "    \n",
    "    if not ads:\n",
    "        print(f\"No ads found on page {i}\")\n",
    "        continue\n",
    "    \n",
    "    # Lấy các liên kết từ thẻ <a>\n",
    "    for ad in ads:\n",
    "        link = ad.find('a', href=True)\n",
    "        if link:\n",
    "            href = link['href']\n",
    "            full_url = f'https://meeyland.com{href}'\n",
    "            all_urls.append(full_url)\n",
    "\n",
    "# Ghi các URL vào một file văn bản\n",
    "file_path = \"buy_urls.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    for url in all_urls:\n",
    "        file.write(url + \"\\n\")\n",
    "\n",
    "print(\"Đã lưu các URL vào file buy_urls.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu các URL vào file rent_urls.txt\n"
     ]
    }
   ],
   "source": [
    "# crawl link -renting\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Khởi tạo một danh sách để lưu trữ các URL\n",
    "all_urls = []\n",
    "\n",
    "# Đặt User-Agent để giả mạo trình duyệt\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Sử dụng vòng lặp để duyệt qua các trang\n",
    "for i in range(1, 15):\n",
    "    url = f'https://meeyland.com/cho-thue-nha-dat?page={i}'\n",
    "    \n",
    "    # Gửi yêu cầu GET đến trang web\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Kiểm tra trạng thái phản hồi\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Unable to access page {i}\")\n",
    "        continue\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm tất cả các thẻ div có class là \"relative slider-hover\"\n",
    "    ads = soup.find_all('div', class_='relative slider-hover')\n",
    "    \n",
    "    if not ads:\n",
    "        print(f\"No ads found on page {i}\")\n",
    "        continue\n",
    "    \n",
    "    # Lấy các liên kết từ thẻ <a>\n",
    "    for ad in ads:\n",
    "        link = ad.find('a', href=True)\n",
    "        if link:\n",
    "            href = link['href']\n",
    "            full_url = f'https://meeyland.com{href}'\n",
    "            all_urls.append(full_url)\n",
    "\n",
    "# Ghi các URL vào một file văn bản\n",
    "file_path = \"rent_urls.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    for url in all_urls:\n",
    "        file.write(url + \"\\n\")\n",
    "\n",
    "print(\"Đã lưu các URL vào file rent_urls.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl data - buying\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Hàm để lấy dữ liệu từ URL và trích xuất thông tin từ HTML\n",
    "def extract_data_from_url(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }  # Set a User-Agent header to mimic a real browser\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "            # Helper function to extract text or return None\n",
    "            def get_text_or_none(element):\n",
    "                return element.text.strip() if element else None\n",
    "\n",
    "            title_element = soup.select_one('h1.text-xl.font-medium')\n",
    "            title = get_text_or_none(title_element)\n",
    "            \n",
    "            address_element = soup.find('p', class_='line-clamp-2 underline lg:no-underline')\n",
    "            address = get_text_or_none(address_element)\n",
    "            \n",
    "            price_element = soup.find('h5', class_='text-black-v8 text-xl font-semibold')\n",
    "            price = get_text_or_none(price_element)            \n",
    "            \n",
    "            description_element = soup.find('div', class_='text-base text-black-v8 leading-[1.875rem]')\n",
    "            allowed_tags = ['br', 'p']\n",
    "            for tag in description_element.find_all(True):  # Tìm tất cả các thẻ con\n",
    "                if tag.name not in allowed_tags:  # Nếu tên thẻ không nằm trong danh sách được phép\n",
    "                    tag.unwrap()  # Loại bỏ thẻ nhưng giữ lại nội dung\n",
    "            description = description_element.decode_contents()\n",
    "            \n",
    "            phone_pattern = r'(\\d{3}[\\s\\.]?\\d{3}[\\s\\.]?\\d{3})'  # Mã số điện thoại có 10 hoặc 11 chữ số\n",
    "            matches = re.findall(phone_pattern, description)\n",
    "\n",
    "            if matches:\n",
    "                phone_number = matches[-1]  # Lấy số điện thoại cuối cùng trong danh sách\n",
    "            else:\n",
    "                phone_number = '0349208325'\n",
    "                \n",
    "            basic_info = {\n",
    "                info.find('p').text.strip().replace(':', ''): get_text_or_none(info.find('p', class_='font-medium line-clamp-2'))\n",
    "                for info in soup.find_all('div', class_='flex text-base text-black-v8 py-2 space-x-8')\n",
    "            }\n",
    "            \n",
    "            # Chỉnh sửa phần diện tích\n",
    "            area = basic_info.get('Diện tích', None)\n",
    "            area = area.split('(')[0].strip() if area else None\n",
    "            \n",
    "            # Lấy link ảnh đầu tiên\n",
    "            image_element = soup.select_one('div.image-fill-wrapper img')\n",
    "            image_link = image_element['src'] if image_element else None\n",
    "            \n",
    "            data = {\n",
    "                'title': title,\n",
    "                'address': address,\n",
    "                'price': price,\n",
    "                'description': description,\n",
    "                'type_estate': basic_info.get('Loại nhà đất', None),\n",
    "                'type': 'Mua Bán',\n",
    "                'area': area,\n",
    "                'bedroom': basic_info.get('Số phòng ngủ', None),\n",
    "                'toilet': basic_info.get('Số phòng tắm/ Toilet', None),\n",
    "                'law': basic_info.get('Giấy tờ pháp lý', None),\n",
    "                'direction': basic_info.get('Hướng nhà', None),\n",
    "                'width': basic_info.get('Mặt tiền', None),\n",
    "                'floor': basic_info.get('Số tầng', None),\n",
    "                'image_links': image_link,\n",
    "                \"phonenumber\": phone_number\n",
    "            }\n",
    "            \n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Lỗi khi truy cập vào URL: {url}, Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Lỗi mạng hoặc yêu cầu: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read URLs from meeyland_urls.txt file\n",
    "with open('buy_urls.txt', 'r') as file:\n",
    "    urls = file.read().splitlines()\n",
    "\n",
    "# Loop through each URL, extract data, and save to the list\n",
    "data_list = []\n",
    "for url in urls:\n",
    "    data = extract_data_from_url(url)\n",
    "    if data:\n",
    "        data_list.append(data)\n",
    "    # time.sleep(2)  # Add a delay between requests to avoid overwhelming the server\n",
    "\n",
    "# Write data to JSON file\n",
    "with open('buy_data.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dữ liệu đã được lưu vào tệp buy_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi trích xuất thông tin từ URL: https://meeyland.com/sieu-hiem-cho-thue-kinh-doanh-kho-xuong-kim-chung-di-trach-hoai-duc-100m2-x5t-chi-10tr-th-1720968648050, Lỗi: 'NoneType' object has no attribute 'find_all'\n",
      "Lỗi khi trích xuất thông tin từ URL: https://meeyland.com/phong-tro-35m2-km-thang-nay-chi-3tr-tai-250-80-phan-trong-tue-thanh-tri-full-do-tang-1tr-1720917476260, Lỗi: 'NoneType' object has no attribute 'find_all'\n",
      "Dữ liệu đã được lưu vào tệp rent_data.json\n"
     ]
    }
   ],
   "source": [
    "#crawl data - renting\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Hàm để lấy dữ liệu từ URL và trích xuất thông tin từ HTML\n",
    "def extract_data_from_url(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }  # Set a User-Agent header to mimic a real browser\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "            # Helper function to extract text or return None\n",
    "            def get_text_or_none(element):\n",
    "                return element.text.strip() if element else None\n",
    "\n",
    "            title_element = soup.select_one('h1.text-xl.font-medium')\n",
    "            title = get_text_or_none(title_element)\n",
    "            \n",
    "            address_element = soup.find('p', class_='line-clamp-2 underline lg:no-underline')\n",
    "            address = get_text_or_none(address_element)\n",
    "            \n",
    "            price_element = soup.find('h5', class_='text-black-v8 text-xl font-semibold')\n",
    "            price = get_text_or_none(price_element)            \n",
    "            \n",
    "            description_element = soup.find('div', class_='text-base text-black-v8 leading-[1.875rem]')\n",
    "            allowed_tags = ['br', 'p']\n",
    "            for tag in description_element.find_all(True):  # Tìm tất cả các thẻ con\n",
    "                if tag.name not in allowed_tags:  # Nếu tên thẻ không nằm trong danh sách được phép\n",
    "                    tag.unwrap()  # Loại bỏ thẻ nhưng giữ lại nội dung\n",
    "            description = description_element.decode_contents()\n",
    "            \n",
    "            phone_pattern = r'(\\d{3}[\\s\\.]?\\d{3}[\\s\\.]?\\d{3})'  # Mã số điện thoại có 10 hoặc 11 chữ số\n",
    "            matches = re.findall(phone_pattern, description)\n",
    "\n",
    "            if matches:\n",
    "                phone_number = matches[-1]  # Lấy số điện thoại cuối cùng trong danh sách\n",
    "            else:\n",
    "                phone_number = '0349208325'\n",
    "                \n",
    "            basic_info = {\n",
    "                info.find('p').text.strip().replace(':', ''): get_text_or_none(info.find('p', class_='font-medium line-clamp-2'))\n",
    "                for info in soup.find_all('div', class_='flex text-base text-black-v8 py-2 space-x-8')\n",
    "            }\n",
    "            \n",
    "            # Chỉnh sửa phần diện tích\n",
    "            area = basic_info.get('Diện tích', None)\n",
    "            area = area.split('(')[0].strip() if area else None\n",
    "            \n",
    "            # Lấy link ảnh đầu tiên\n",
    "            image_element = soup.select_one('div.image-fill-wrapper img')\n",
    "            image_link = image_element['src'] if image_element else None\n",
    "            \n",
    "            data = {\n",
    "                'title': title,\n",
    "                'address': address,\n",
    "                'price': price,\n",
    "                'description': description,\n",
    "                'type_estate': basic_info.get('Loại nhà đất', None),\n",
    "                'type': 'Cho Thuê',\n",
    "                'area': area,\n",
    "                'bedroom': basic_info.get('Số phòng ngủ', None),\n",
    "                'toilet': basic_info.get('Số phòng tắm/ Toilet', None),\n",
    "                'law': basic_info.get('Giấy tờ pháp lý', None),\n",
    "                'direction': basic_info.get('Hướng nhà', None),\n",
    "                'width': basic_info.get('Mặt tiền', None),\n",
    "                'floor': basic_info.get('Số tầng', None),\n",
    "                'image_links': image_link,\n",
    "                'phonenumber': phone_number\n",
    "            }\n",
    "            \n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Lỗi khi truy cập vào URL: {url}, Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Lỗi mạng hoặc yêu cầu: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read URLs from meeyland_urls.txt file\n",
    "with open('rent_urls.txt', 'r') as file:\n",
    "    urls = file.read().splitlines()\n",
    "\n",
    "# Loop through each URL, extract data, and save to the list\n",
    "data_list = []\n",
    "for url in urls:\n",
    "    data = extract_data_from_url(url)\n",
    "    if data:\n",
    "        data_list.append(data)\n",
    "\n",
    "# Write data to JSON file\n",
    "with open('rent_data.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dữ liệu đã được lưu vào tệp rent_data.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
