{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu các URL vào file buy_urls.txt\n"
     ]
    }
   ],
   "source": [
    "# crawl link buy\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Khởi tạo một danh sách để lưu trữ các URL\n",
    "all_urls = []\n",
    "\n",
    "# Sử dụng vòng lặp để duyệt qua các trang\n",
    "for i in range(1, 5):  # Bạn có thể thay đổi số lượng trang tùy ý\n",
    "    url = f'https://alonhadat.com.vn/nha-dat/can-ban/trang--{i}.html'\n",
    "    \n",
    "    # Gửi yêu cầu GET đến trang web\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Kiểm tra xem yêu cầu có thành công không\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Không thể truy cập trang {url}\")\n",
    "        continue\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm tất cả các thẻ <a> trong <div class=\"content-item\">\n",
    "    links = soup.select('div.content-item div.ct_title a')\n",
    "\n",
    "    # Lặp qua các liên kết và thêm vào danh sách\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and href.startswith('/'):  # Đảm bảo liên kết là đường dẫn tương đối\n",
    "            full_link = f\"https://alonhadat.com.vn{href}\"\n",
    "            all_urls.append(full_link)\n",
    "\n",
    "# Ghi các URL vào một file văn bản\n",
    "file_path = \"buy_urls.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    for url in all_urls:\n",
    "        file.write(url + \"\\n\")\n",
    "\n",
    "print(f\"Đã lưu các URL vào file {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu các URL vào file rent_urls.txt\n"
     ]
    }
   ],
   "source": [
    "# crawl link - rent \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Khởi tạo một danh sách để lưu trữ các URL\n",
    "all_urls = []\n",
    "\n",
    "# Sử dụng vòng lặp để duyệt qua các trang\n",
    "for i in range(1, 5):  # Bạn có thể thay đổi số lượng trang tùy ý\n",
    "    url = f'https://alonhadat.com.vn/nha-dat/cho-thue/trang--{i}.html'\n",
    "    \n",
    "    # Gửi yêu cầu GET đến trang web\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Kiểm tra xem yêu cầu có thành công không\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Không thể truy cập trang {url}\")\n",
    "        continue\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm tất cả các thẻ <a> trong <div class=\"content-item\">\n",
    "    links = soup.select('div.content-item div.ct_title a')\n",
    "\n",
    "    # Lặp qua các liên kết và thêm vào danh sách\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and href.startswith('/'):  # Đảm bảo liên kết là đường dẫn tương đối\n",
    "            full_link = f\"https://alonhadat.com.vn{href}\"\n",
    "            all_urls.append(full_link)\n",
    "\n",
    "# Ghi các URL vào một file văn bản\n",
    "file_path = \"rent_urls.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    for url in all_urls:\n",
    "        file.write(url + \"\\n\")\n",
    "\n",
    "print(f\"Đã lưu các URL vào file {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được lưu vào tệp data.json\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_text(soup, selector):\n",
    "    element = soup.select_one(selector)\n",
    "    return element.text.strip() if element else None\n",
    "\n",
    "def extract_text_from_td(td_elements, label):\n",
    "    for i in range(len(td_elements) - 1):\n",
    "        if td_elements[i].text.strip() == label:\n",
    "            return td_elements[i + 1].text.strip()\n",
    "    return None\n",
    "\n",
    "def replace_underscore_with_null(value):\n",
    "    return None if value == \"_\" or value == \"---\" else value\n",
    "\n",
    "def process_address(address):\n",
    "    parts = address.split(',')\n",
    "    if len(parts) >= 3:\n",
    "        city = parts[-1].strip()\n",
    "        district = parts[-2].strip()\n",
    "        full_address = ', '.join(parts[:-2]).strip()\n",
    "        combined_address = f\"{full_address}, {district}, {city}\"\n",
    "        return combined_address, city, district\n",
    "    return address, None, None\n",
    "\n",
    "def extract_phone_number(soup):\n",
    "    try:\n",
    "        phone_element = soup.select_one(\"div.fone a\")\n",
    "        if phone_element and phone_element.has_attr('href') and phone_element['href'].startswith('tel:'):\n",
    "            phone_number = phone_element['href'].replace('tel:', '').strip()\n",
    "            return phone_number\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất số điện thoại: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_data_from_url(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url.strip(), headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            title = extract_text(soup, \"div.title h1\")\n",
    "\n",
    "            detail = soup.select_one(\"div.detail.text-content\")\n",
    "            if detail:\n",
    "                # Lấy nội dung văn bản và loại bỏ khoảng trắng đầu cuối\n",
    "                detail_text = detail.get_text(strip=True)\n",
    "                \n",
    "                # Loại bỏ các ký tự không mong muốn như \\r\n",
    "                detail_text = re.sub(r'[^\\w\\s,.()\\–]', '', detail_text)\n",
    "                detail_text = detail_text.replace('\\r', '')\n",
    "            else:\n",
    "                detail_text = None\n",
    "\n",
    "            price = extract_text(soup, \"span.price .value\")\n",
    "            area = extract_text(soup, \"span.square .value\")\n",
    "\n",
    "            address = extract_text(soup, \"div.address .value\")\n",
    "            combined_address, city, district = process_address(address)\n",
    "\n",
    "            more_info = soup.select(\"div.moreinfor1 .infor td\")\n",
    "\n",
    "            property_id = extract_text_from_td(more_info, \"Mã tin\")\n",
    "            direction = extract_text_from_td(more_info, \"Hướng\")\n",
    "            type_of_listing = extract_text_from_td(more_info, \"Loại tin\")\n",
    "            road_width = extract_text_from_td(more_info, \"Đường trước nhà\")\n",
    "            bedrooms = extract_text_from_td(more_info, \"Số phòng ngủ\")\n",
    "            floors = extract_text_from_td(more_info, \"Số lầu\")\n",
    "            legal_status = extract_text_from_td(more_info, \"Pháp lý\")\n",
    "            property_type = extract_text_from_td(more_info, \"Loại BDS\")\n",
    "            width = extract_text_from_td(more_info, \"Chiều ngang\")\n",
    "            length = extract_text_from_td(more_info, \"Chiều dài\")\n",
    "\n",
    "            image = soup.select_one(\"div.images img\")\n",
    "            image_link = image['src'] if image else None\n",
    "            full_image_link = \"https://alonhadat.com.vn/\" + image_link if image_link else None\n",
    "            \n",
    "            # Thay thế giá trị \"_\" bằng null\n",
    "            property_id = replace_underscore_with_null(property_id)\n",
    "            direction = replace_underscore_with_null(direction)\n",
    "            type_of_listing = replace_underscore_with_null(type_of_listing)\n",
    "            road_width = replace_underscore_with_null(road_width)\n",
    "            bedrooms = replace_underscore_with_null(bedrooms)\n",
    "            floors = replace_underscore_with_null(floors)\n",
    "            legal_status = replace_underscore_with_null(legal_status)\n",
    "            property_type = replace_underscore_with_null(property_type)\n",
    "            width = replace_underscore_with_null(width)\n",
    "            length = replace_underscore_with_null(length)\n",
    "\n",
    "            # Trích xuất số điện thoại\n",
    "            phone_number = extract_phone_number(soup)\n",
    "\n",
    "            if full_image_link is None:\n",
    "                return None\n",
    "            \n",
    "            return {\n",
    "                \"title\": title or None,\n",
    "                \"description\": detail_text or None,\n",
    "                \"price\": price or None,\n",
    "                \"area\": area or None,\n",
    "                \"address\": combined_address or None,\n",
    "                \"province_city\": city or None,\n",
    "                \"district\": district or None,\n",
    "                \"direction\": direction,\n",
    "                \"type\": 'Mua Bán',\n",
    "                \"bedroom\": bedrooms,\n",
    "                \"toilet\": bedrooms, \n",
    "                \"floor\": floors,\n",
    "                \"law\": legal_status,\n",
    "                \"type_estate\": property_type,\n",
    "                \"width\": width,\n",
    "                \"length\": length,\n",
    "                'image_links': full_image_link,                \n",
    "                \"phonenumber\": phone_number\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Lỗi khi truy cập vào URL: {url}\")\n",
    "            return None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout khi truy cập vào URL: {url}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Lỗi kết nối khi truy cập vào URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with open(\"buy_urls.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    urls = file.readlines()\n",
    "\n",
    "for url in urls:\n",
    "    url = url.strip()\n",
    "    item = crawl_data_from_url(url)\n",
    "    if item:\n",
    "        data_list.append(item)\n",
    "\n",
    "with open(\"buy_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dữ liệu đã được lưu vào tệp buy_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def extract_text(soup, selector):\n",
    "    element = soup.select_one(selector)\n",
    "    return element.text.strip() if element else None\n",
    "\n",
    "def extract_text_from_td(td_elements, label):\n",
    "    for i in range(len(td_elements) - 1):\n",
    "        if td_elements[i].text.strip() == label:\n",
    "            return td_elements[i + 1].text.strip()\n",
    "    return None\n",
    "\n",
    "def replace_underscore_with_null(value):\n",
    "    return None if value == \"_\" or value == \"---\" else value\n",
    "\n",
    "def process_address(address):\n",
    "    parts = address.split(',')\n",
    "    if len(parts) >= 3:\n",
    "        city = parts[-1].strip()\n",
    "        district = parts[-2].strip()\n",
    "        full_address = ', '.join(parts[:-2]).strip()\n",
    "        combined_address = f\"{full_address}, {district}, {city}\"\n",
    "        return combined_address, city, district\n",
    "    return address, None, None\n",
    "\n",
    "def extract_phone_number(soup):\n",
    "    try:\n",
    "        phone_element = soup.select_one(\"div.fone a\")\n",
    "        if phone_element and phone_element.has_attr('href') and phone_element['href'].startswith('tel:'):\n",
    "            phone_number = phone_element['href'].replace('tel:', '').strip()\n",
    "            return phone_number\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất số điện thoại: {e}\")\n",
    "        return None\n",
    "    \n",
    "def crawl_data_from_url(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url.strip(), headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            title = extract_text(soup, \"div.title h1\")\n",
    "            title = re.sub(r'[^\\w\\s,.()\\–]', '', title)\n",
    "\n",
    "            detail = soup.select_one(\"div.detail.text-content\")\n",
    "            if detail:\n",
    "                # Lấy nội dung văn bản và loại bỏ khoảng trắng đầu cuối\n",
    "                detail_text = detail.get_text(strip=True)\n",
    "                \n",
    "                # Loại bỏ các ký tự không mong muốn như \\r\n",
    "                detail_text = re.sub(r'[^\\w\\s,.()\\–]', '', detail_text)\n",
    "                detail_text = detail_text.replace('\\r', '')\n",
    "            else:\n",
    "                detail_text = None\n",
    "\n",
    "            price = extract_text(soup, \"span.price .value\")\n",
    "            area = extract_text(soup, \"span.square .value\")\n",
    "\n",
    "            address = extract_text(soup, \"div.address .value\")\n",
    "            combined_address, city, district = process_address(address)\n",
    "\n",
    "            more_info = soup.select(\"div.moreinfor1 .infor td\")\n",
    "\n",
    "            property_id = extract_text_from_td(more_info, \"Mã tin\")\n",
    "            direction = extract_text_from_td(more_info, \"Hướng\")\n",
    "            type_of_listing = extract_text_from_td(more_info, \"Loại tin\")\n",
    "            road_width = extract_text_from_td(more_info, \"Đường trước nhà\")\n",
    "            bedrooms = extract_text_from_td(more_info, \"Số phòng ngủ\")\n",
    "            floors = extract_text_from_td(more_info, \"Số lầu\")\n",
    "            legal_status = extract_text_from_td(more_info, \"Pháp lý\")\n",
    "            property_type = extract_text_from_td(more_info, \"Loại BDS\")\n",
    "            width = extract_text_from_td(more_info, \"Chiều ngang\")\n",
    "            length = extract_text_from_td(more_info, \"Chiều dài\")\n",
    "\n",
    "            image = soup.select_one(\"div.images img\")\n",
    "            image_link = image['src'] if image else None\n",
    "            full_image_link = \"https://alonhadat.com.vn/\" + image_link if image_link else None\n",
    "            \n",
    "            \n",
    "            # Thay thế giá trị \"_\" bằng null\n",
    "            property_id = replace_underscore_with_null(property_id)\n",
    "            direction = replace_underscore_with_null(direction)\n",
    "            type_of_listing = replace_underscore_with_null(type_of_listing)\n",
    "            road_width = replace_underscore_with_null(road_width)\n",
    "            bedrooms = replace_underscore_with_null(bedrooms)\n",
    "            floors = replace_underscore_with_null(floors)\n",
    "            legal_status = replace_underscore_with_null(legal_status)\n",
    "            property_type = replace_underscore_with_null(property_type)\n",
    "            width = replace_underscore_with_null(width)\n",
    "            length = replace_underscore_with_null(length)\n",
    "\n",
    "            if full_image_link is None:\n",
    "                return None\n",
    "            \n",
    "            phone_number = extract_phone_number(soup)\n",
    "            \n",
    "            return {\n",
    "                \"title\": title or None,\n",
    "                \"description\": detail_text or None,\n",
    "                \"price\": price or None,\n",
    "                \"area\": area or None,\n",
    "                \"address\": combined_address or None,\n",
    "                \"province_city\": city or None,\n",
    "                \"district\": district or None,\n",
    "                \"direction\": direction,\n",
    "                \"type\": 'Cho Thuê',\n",
    "                \"bedroom\": bedrooms,\n",
    "                \"toilet\":bedrooms,\n",
    "                \"floor\": floors,\n",
    "                \"law\": legal_status,\n",
    "                \"type_estate\": property_type,\n",
    "                \"width\": width,\n",
    "                'image_links': full_image_link,\n",
    "                'phonenumber': phone_number\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Lỗi khi truy cập vào URL: {url}\")\n",
    "            return None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout khi truy cập vào URL: {url}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Lỗi kết nối khi truy cập vào URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with open(\"rent_urls.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    urls = file.readlines()\n",
    "\n",
    "for url in urls:\n",
    "    url = url.strip()\n",
    "    item = crawl_data_from_url(url)\n",
    "    if item:\n",
    "        data_list.append(item)\n",
    "\n",
    "with open(\"rent_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dữ liệu đã được lưu vào tệp rent_data.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
