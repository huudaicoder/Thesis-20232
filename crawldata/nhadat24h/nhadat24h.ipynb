{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu các URL vào file buy_urls.txt\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Khởi tạo một danh sách để lưu trữ các URL\n",
    "all_urls = []\n",
    "\n",
    "# Sử dụng vòng lặp để duyệt qua các trang\n",
    "for i in range(1, 5):\n",
    "    url = f'https://nhadat24h.net/nha-dat-ban/page{i}'\n",
    "    \n",
    "    # Gửi yêu cầu GET đến trang web\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm tất cả các thẻ <a> có class là \"title\" và thuộc tính href\n",
    "    links = soup.find_all('a', class_='a-title', href=True)\n",
    "    \n",
    "    # Kiểm tra xem có ít nhất một thẻ a nào không\n",
    "    if links:\n",
    "        # Lặp qua các thẻ a và lấy thuộc tính href\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            full_url = 'https://nhadat24h.net' + href\n",
    "            all_urls.append(full_url)\n",
    "            \n",
    "# Ghi các URL vào một file văn bản\n",
    "file_path = \"buy_urls.txt\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for url in all_urls:\n",
    "        file.write(url + \"\\n\")\n",
    "\n",
    "print(\"Đã lưu các URL vào file buy_urls.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu các URL vào file rent_urls.txt\n"
     ]
    }
   ],
   "source": [
    "#crawl rent link\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Khởi tạo một danh sách để lưu trữ các URL\n",
    "all_urls = []\n",
    "\n",
    "# Sử dụng vòng lặp để duyệt qua các trang\n",
    "for i in range(1, 5):\n",
    "    url = f'https://nhadat24h.net/nha-dat-cho-thue/page{i}'\n",
    "    \n",
    "    # Gửi yêu cầu GET đến trang web\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm tất cả các thẻ <a> có class là \"title\" và thuộc tính href\n",
    "    links = soup.find_all('a', class_='a-title', href=True)\n",
    "    \n",
    "    # Kiểm tra xem có ít nhất một thẻ a nào không\n",
    "    if links:\n",
    "        # Lặp qua các thẻ a và lấy thuộc tính href\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            full_url = 'https://nhadat24h.net' + href\n",
    "            all_urls.append(full_url)\n",
    "            \n",
    "# Ghi các URL vào một file văn bản\n",
    "file_path = \"rent_urls.txt\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for url in all_urls:\n",
    "        file.write(url + \"\\n\")\n",
    "\n",
    "print(\"Đã lưu các URL vào file rent_urls.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được lưu vào tệp buy_data.json\n"
     ]
    }
   ],
   "source": [
    "# crawl data\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def extract_text(soup, selector):\n",
    "    element = soup.select_one(selector)\n",
    "    return element.text.strip() if element else None\n",
    "\n",
    "def extract_image_url(soup):\n",
    "    image_element = soup.select_one(\"a.swipebox\")\n",
    "    if image_element:\n",
    "        base_url = \"https://nhadat24h.net/\"\n",
    "        image_url = image_element['href']\n",
    "        return base_url + image_url\n",
    "    return None\n",
    "\n",
    "def extract_description(soup):\n",
    "    description_element = soup.select_one(\"div.dv-txt-mt.dv-pt-item\")\n",
    "    if description_element:\n",
    "        # Extract the text and remove any additional tags\n",
    "        description = ' '.join(description_element.stripped_strings)\n",
    "        return description\n",
    "    return None\n",
    "\n",
    "def crawl_data_from_url(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url.strip(), headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            title = extract_text(soup, \"h1#txtcontenttieudetin\")\n",
    "\n",
    "            # Trích xuất giá và diện tích\n",
    "            price_area_element = soup.select_one(\"label#ContentPlaceHolder1_ctl00_lbGiaDienTich\")\n",
    "            if price_area_element:\n",
    "                price_area_text = price_area_element.get_text(strip=True)\n",
    "                price_area_parts = price_area_text.split('-')\n",
    "                if len(price_area_parts) == 2:\n",
    "                    price = price_area_parts[0].strip().split(':')[1].strip()\n",
    "                    area = price_area_parts[1].strip().split('M²')[0].strip()\n",
    "                    \n",
    "            # Trích xuất tình trạng pháp lý\n",
    "            legal_status = None\n",
    "            for p in soup.find_all('p'):\n",
    "                if 'Tình trạng pháp lý' in p.get_text():\n",
    "                    strong_tag = p.find('strong')\n",
    "                    if strong_tag:\n",
    "                        legal_status = strong_tag.text.strip()\n",
    "                        break\n",
    "\n",
    "            property_type = extract_text(soup, \"i.fas.fa-building + label\")\n",
    "            address = extract_text(soup, \"label#ContentPlaceHolder1_ctl00_lbTinhThanh\")\n",
    "            bedrooms = extract_text(soup, \"td.col1:-soup-contains('Phòng Ngủ') + td\")\n",
    "            bathrooms = extract_text(soup, \"td.col1:-soup-contains('Phòng WC') + td\")\n",
    "            floors = extract_text(soup, \"td.col1:-soup-contains('Số tầng') + td\")\n",
    "            direction = extract_text(soup, \"td.col1:-soup-contains('Hướng') + td\")\n",
    "            entrance_width = extract_text(soup, \"td.col1:-soup-contains('Đường vào') + td\")\n",
    "            frontage = extract_text(soup, \"td.col1:-soup-contains('Mặt tiền') + td\")\n",
    "            \n",
    "            # Trích xuất link ảnh\n",
    "            image_url = extract_image_url(soup)\n",
    "            image_links = image_url if image_url else None\n",
    "\n",
    "            # Trích xuất mô tả\n",
    "            description = extract_description(soup)\n",
    "\n",
    "            # Trích xuất thông tin về tỉnh/thành phố và quận/huyện từ URL\n",
    "            city_tag = soup.select_one(\"#loadmenuTinhThanh\")\n",
    "            city = city_tag.text.strip() if city_tag else None\n",
    "\n",
    "            breadcrumbs = soup.select(\"div.dv-breadcrumb li\")\n",
    "            district = None\n",
    "            for breadcrumb in breadcrumbs:\n",
    "                a_tag = breadcrumb.find(\"a\")\n",
    "                if a_tag and (\"Quận\" in a_tag.text or \"Huyện\" in a_tag.text or \"Thị xã\" in a_tag.text or \"Thành phố\" in a_tag.text):\n",
    "                    district = a_tag.text.strip()\n",
    "                    \n",
    "                    # Remove \"Quận\", \"Huyện\", \"Thị xã\", \"Thành phố\"\n",
    "                    district = district.replace(\"Quận\", \"\").replace(\"Huyện\", \"\").replace(\"Thị xã\", \"\").replace(\"Thành phố\", \"\").strip()\n",
    "                    \n",
    "                    break\n",
    "\n",
    "            data = {\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"area\": area,\n",
    "                \"type\":\"Mua Bán\",\n",
    "                \"address\": address,\n",
    "                \"description\": description,\n",
    "                \"law\": legal_status,\n",
    "                \"type_estate\": property_type,\n",
    "                \"province\": city,\n",
    "                \"district\": district,\n",
    "                \"bedroom\": bedrooms,\n",
    "                \"toilet\": bathrooms,\n",
    "                \"floor\": floors,\n",
    "                \"direction\": direction,\n",
    "                \"entrance_width\": entrance_width,\n",
    "                \"width\": frontage,\n",
    "                \"image_links\": image_links                \n",
    "            }\n",
    "\n",
    "            # Chuyển các trường có giá trị là chuỗi rỗng thành null\n",
    "            for key, value in data.items():\n",
    "                if value == \"\":\n",
    "                    data[key] = None\n",
    "\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Lỗi khi truy cập vào URL: {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with open(\"buy_urls.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    urls = file.readlines()\n",
    "\n",
    "for url in urls:\n",
    "    url = url.strip()\n",
    "    item = crawl_data_from_url(url)\n",
    "    if item:\n",
    "        data_list.append(item)\n",
    "\n",
    "with open(\"buy_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dữ liệu đã được lưu vào tệp buy_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl rent data\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def extract_text(soup, selector):\n",
    "    element = soup.select_one(selector)\n",
    "    return element.text.strip() if element else None\n",
    "\n",
    "def extract_image_url(soup):\n",
    "    image_element = soup.select_one(\"a.swipebox\")\n",
    "    if image_element:\n",
    "        base_url = \"https://nhadat24h.net/\"\n",
    "        image_url = image_element['href']\n",
    "        return base_url + image_url\n",
    "    return None\n",
    "\n",
    "def extract_description(soup):\n",
    "    description_element = soup.select_one(\"div.dv-txt-mt.dv-pt-item\")\n",
    "    if description_element:\n",
    "        # Extract the text and remove any additional tags\n",
    "        description = ' '.join(description_element.stripped_strings)\n",
    "        return description\n",
    "    return None\n",
    "\n",
    "def crawl_data_from_url(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url.strip(), headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            title = extract_text(soup, \"h1#txtcontenttieudetin\")\n",
    "\n",
    "            # Trích xuất giá và diện tích\n",
    "            price_area_element = soup.select_one(\"label#ContentPlaceHolder1_ctl00_lbGiaDienTich\")\n",
    "            if price_area_element:\n",
    "                price_area_text = price_area_element.get_text(strip=True)\n",
    "                price_area_parts = price_area_text.split('-')\n",
    "                if len(price_area_parts) == 2:\n",
    "                    price = price_area_parts[0].strip().split(':')[1].strip()\n",
    "                    area = price_area_parts[1].strip().split('M²')[0].strip()\n",
    "                    \n",
    "            # Trích xuất tình trạng pháp lý\n",
    "            legal_status = None\n",
    "            for p in soup.find_all('p'):\n",
    "                if 'Tình trạng pháp lý' in p.get_text():\n",
    "                    strong_tag = p.find('strong')\n",
    "                    if strong_tag:\n",
    "                        legal_status = strong_tag.text.strip()\n",
    "                        break\n",
    "\n",
    "            property_type = extract_text(soup, \"i.fas.fa-building + label\")\n",
    "            address = extract_text(soup, \"label#ContentPlaceHolder1_ctl00_lbTinhThanh\")\n",
    "            bedrooms = extract_text(soup, \"td.col1:-soup-contains('Phòng Ngủ') + td\")\n",
    "            bathrooms = extract_text(soup, \"td.col1:-soup-contains('Phòng WC') + td\")\n",
    "            floors = extract_text(soup, \"td.col1:-soup-contains('Số tầng') + td\")\n",
    "            direction = extract_text(soup, \"td.col1:-soup-contains('Hướng') + td\")\n",
    "            entrance_width = extract_text(soup, \"td.col1:-soup-contains('Đường vào') + td\")\n",
    "            frontage = extract_text(soup, \"td.col1:-soup-contains('Mặt tiền') + td\")\n",
    "            \n",
    "            # Trích xuất link ảnh\n",
    "            image_url = extract_image_url(soup)\n",
    "            image_links = image_url if image_url else None\n",
    "\n",
    "            # Trích xuất mô tả\n",
    "            description = extract_description(soup)\n",
    "\n",
    "            # Trích xuất thông tin về tỉnh/thành phố và quận/huyện từ URL\n",
    "            city_tag = soup.select_one(\"#loadmenuTinhThanh\")\n",
    "            city = city_tag.text.strip() if city_tag else None\n",
    "\n",
    "            breadcrumbs = soup.select(\"div.dv-breadcrumb li\")\n",
    "            district = None\n",
    "            for breadcrumb in breadcrumbs:\n",
    "                a_tag = breadcrumb.find(\"a\")\n",
    "                if a_tag and (\"Quận\" in a_tag.text or \"Huyện\" in a_tag.text or \"Thị xã\" in a_tag.text or \"Thành phố\" in a_tag.text):\n",
    "                    district = a_tag.text.strip()\n",
    "                    \n",
    "                    # Remove \"Quận\", \"Huyện\", \"Thị xã\", \"Thành phố\"\n",
    "                    district = district.replace(\"Quận\", \"\").replace(\"Huyện\", \"\").replace(\"Thị xã\", \"\").replace(\"Thành phố\", \"\").strip()\n",
    "                    \n",
    "                    break\n",
    "\n",
    "            data = {\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"area\": area,\n",
    "                \"type\":\"Cho Thuê\",\n",
    "                \"address\": address,\n",
    "                \"description\": description,\n",
    "                \"law\": legal_status,\n",
    "                \"type_estate\": property_type,\n",
    "                \"province\": city,\n",
    "                \"district\": district,\n",
    "                \"bedroom\": bedrooms,\n",
    "                \"toilet\": bathrooms,\n",
    "                \"floor\": floors,\n",
    "                \"direction\": direction,\n",
    "                # \"entrance_width\": entrance_width,\n",
    "                \"width\": frontage,\n",
    "                \"image_links\": image_links                \n",
    "            }\n",
    "\n",
    "            # Chuyển các trường có giá trị là chuỗi rỗng thành null\n",
    "            for key, value in data.items():\n",
    "                if value == \"\":\n",
    "                    data[key] = None\n",
    "\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Lỗi khi truy cập vào URL: {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with open(\"rent_urls.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    urls = file.readlines()\n",
    "\n",
    "for url in urls:\n",
    "    url = url.strip()\n",
    "    item = crawl_data_from_url(url)\n",
    "    if item:\n",
    "        data_list.append(item)\n",
    "\n",
    "with open(\"rent_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dữ liệu đã được lưu vào tệp rent_data.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
