{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl link - buying\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Khởi tạo một danh sách để lưu trữ các URL\n",
    "all_urls = []\n",
    "\n",
    "# Sử dụng vòng lặp để duyệt qua các trang\n",
    "for i in range(1, 10):\n",
    "    url = f'https://muaban.net/bat-dong-san/ban-nha-dat-chung-cu#page={i}'\n",
    "    \n",
    "    # Gửi yêu cầu GET đến trang web\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm tất cả các thẻ <a> có class là \"title\" và thuộc tính href\n",
    "    links = soup.find_all('a', class_='title', href=True)\n",
    "    \n",
    "    # Kiểm tra xem có ít nhất một thẻ a nào không\n",
    "    if links:\n",
    "        # Lặp qua các thẻ a và lấy thuộc tính href\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            full_url = 'https://muaban.net' + href\n",
    "            all_urls.append(full_url)\n",
    "    \n",
    "# Ghi các URL vào một file văn bản\n",
    "file_path = \"buy_urls.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    for url in all_urls:\n",
    "        file.write(url + \"\\n\")\n",
    "\n",
    "print(\"Đã lưu các URL vào file buy_urls.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl link - renting\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Khởi tạo một danh sách để lưu trữ các URL\n",
    "all_urls = []\n",
    "\n",
    "# Sử dụng vòng lặp để duyệt qua các trang\n",
    "for i in range(1, 7):\n",
    "    url = f'https://muaban.net/bat-dong-san/cho-thue-nha-dat?page={i}'\n",
    "    \n",
    "    # Gửi yêu cầu GET đến trang web\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Tìm tất cả các thẻ <a> có class là \"title\" và thuộc tính href\n",
    "    links = soup.find_all('a', class_='title', href=True)\n",
    "    \n",
    "    # Kiểm tra xem có ít nhất một thẻ a nào không\n",
    "    if links:\n",
    "        # Lặp qua các thẻ a và lấy thuộc tính href\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            full_url = 'https://muaban.net' + href\n",
    "            all_urls.append(full_url)\n",
    "    \n",
    "# Ghi các URL vào một file văn bản\n",
    "file_path = \"rent_urls.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    for url in all_urls:\n",
    "        file.write(url + \"\\n\")\n",
    "\n",
    "print(\"Đã lưu các URL vào file rent_urls.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được lưu vào tệp buy_data.json\n"
     ]
    }
   ],
   "source": [
    "# crawl data - buying\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def extract_info_from_url(url):\n",
    "    try:\n",
    "        # Thực hiện yêu cầu GET để truy cập vào URL\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            html_content = response.text\n",
    "\n",
    "            # Sử dụng BeautifulSoup để phân tích HTML\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            # Trích xuất thông tin cần thiết từ HTML\n",
    "            title = soup.find(\"h1\").text.strip()\n",
    "            price = soup.find(class_=\"price\").text.strip()\n",
    "            address = soup.find(class_=\"address\").text.strip()\n",
    "\n",
    "            # Trích xuất thông tin từ các thẻ <ul> và <li>\n",
    "            ul_element = soup.find(\"ul\", class_=\"sc-6orc5o-24 foVitw\")\n",
    "\n",
    "            # Khởi tạo các biến thông tin\n",
    "            property_type, area, legal_documents, floors, bedrooms, toilets, direction = None, None, None, None, None, None, None\n",
    "\n",
    "            # Nếu có <ul> chứa thông tin, trích xuất thông tin từ các phần tử <span>\n",
    "            if ul_element:\n",
    "                items = ul_element.find_all(\"li\")\n",
    "                for item in items:\n",
    "                    label = item.find(\"span\", class_=\"label\")\n",
    "                    if label:\n",
    "                        span = item.find(\"span\", string=True, recursive=False)\n",
    "                        if span:\n",
    "                            if \"Loại hình bất động sản\" in label.text:\n",
    "                                property_type = span.text.strip()\n",
    "                            elif \"Diện tích đất\" in label.text:\n",
    "                                area = span.text.strip()\n",
    "                            elif \"Diện tích sử dụng\" in label.text:\n",
    "                                area = span.text.strip()\n",
    "                            elif \"Giấy tờ pháp lý\" in label.text:\n",
    "                                legal_documents = span.text.strip()\n",
    "                            elif \"Tổng số tầng\" in label.text:\n",
    "                                floors = span.text.strip()\n",
    "                            elif \"Số phòng ngủ\" in label.text:\n",
    "                                bedrooms = span.text.strip()\n",
    "                            elif \"Số phòng vệ sinh\" in label.text:\n",
    "                                toilets = span.text.strip()\n",
    "                            elif \"Hướng cửa chính\" in label.text:\n",
    "                                direction = span.text.strip()\n",
    "\n",
    "            # Trích xuất thông tin \"Chi tiết\"\n",
    "            detail_div = soup.find(\"div\", class_=\"sc-6orc5o-18 gdAVnx\") \n",
    "\n",
    "            for child in detail_div.find_all(\"div\", class_=\"phone-wrapper\"):\n",
    "                child.decompose()  \n",
    "           \n",
    "            for tag in detail_div.find_all(True):  # Tìm tất cả các thẻ con\n",
    "                if tag.name != 'br':  # Nếu tên thẻ không phải là <br>\n",
    "                    tag.unwrap()  # Giữ lại nội dung của thẻ và loại bỏ thẻ đó\n",
    "\n",
    "            details = detail_div.decode_contents()\n",
    "                        \n",
    "            phone_wrapper = soup.find(\"div\", class_=\"phone-wrapper\")\n",
    "            if phone_wrapper:\n",
    "                phone_span = phone_wrapper.find(\"span\", class_=\"phone-hidden\")\n",
    "                if phone_span and 'data-phone' in phone_span.attrs:\n",
    "                    phone_number = phone_span['data-phone']\n",
    "        \n",
    "            # Trích xuất link ảnh\n",
    "            image_div = soup.select_one(\"div.slick-slide.slick-active.slick-current img\")\n",
    "            image_link = None\n",
    "            if image_div and 'src' in image_div.attrs:\n",
    "                image_link = image_div['src']\n",
    "            \n",
    "            if image_link is None or area is None:\n",
    "                return None\n",
    "            \n",
    "            # Tạo một từ điển chứa thông tin từng bài đăng\n",
    "            item = {\n",
    "                \"title\": title,\n",
    "                \"price\": price,                \n",
    "                \"area\": area,\n",
    "                \"address\": address,\n",
    "                \"description\": details,\n",
    "                \"type\": \"Mua Bán\",\n",
    "                \"type_estate\": property_type,\n",
    "                \"law\": legal_documents,\n",
    "                \"floor\": floors,\n",
    "                \"bedroom\": bedrooms,\n",
    "                \"toilet\": toilets,             \n",
    "                \"direction\": direction,   \n",
    "                \"image_links\": image_link,\n",
    "                \"phonenumber\": phone_number\n",
    "            }\n",
    "            return item\n",
    "        else:\n",
    "            print(f\"Lỗi khi truy cập vào URL: {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "\n",
    "# Tạo một danh sách để lưu trữ thông tin từ mỗi URL\n",
    "data_list = []\n",
    "\n",
    "# Đọc nội dung từ tệp muaban.txt\n",
    "with open(\"buy_urls.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    urls = file.readlines()\n",
    "\n",
    "# Lặp qua từng URL để lấy thông tin\n",
    "for url in urls:\n",
    "    # Xóa khoảng trắng và ký tự xuống dòng từ URL\n",
    "    url = url.strip()\n",
    "    # Trích xuất thông tin từ URL và thêm vào danh sách\n",
    "    item = extract_info_from_url(url)\n",
    "    if item:\n",
    "        data_list.append(item)\n",
    "\n",
    "# Lưu danh sách dưới dạng JSON vào tệp muaban.json\n",
    "with open(\"buy_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dữ liệu đã được lưu vào tệp buy_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl data - renting\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def extract_info_from_url(url):\n",
    "    try:\n",
    "        # Thực hiện yêu cầu GET để truy cập vào URL\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            html_content = response.text\n",
    "\n",
    "            # Sử dụng BeautifulSoup để phân tích HTML\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "            \n",
    "                       \n",
    "            # Trích xuất thông tin cần thiết từ HTML\n",
    "            title = soup.find(\"h1\").text.strip()\n",
    "            price = soup.find(class_=\"price\").text.strip()\n",
    "            address = soup.find(class_=\"address\").text.strip()\n",
    "            # date_posted = soup.find(class_=\"date\").text.strip()\n",
    "\n",
    "            # Trích xuất thông tin từ các thẻ <ul> và <li>\n",
    "            ul_element = soup.find(\"ul\", class_=\"sc-6orc5o-24 foVitw\")\n",
    "\n",
    "            # Khởi tạo các biến thông tin\n",
    "            property_type, area, legal_documents, floors, bedrooms, toilets, direction = None, None, None, None, None, None, None\n",
    "\n",
    "            # Nếu có <ul> chứa thông tin, trích xuất thông tin từ các phần tử <span>\n",
    "            if ul_element:\n",
    "                items = ul_element.find_all(\"li\")\n",
    "                for item in items:\n",
    "                    label = item.find(\"span\", class_=\"label\")\n",
    "                    if label:\n",
    "                        span = item.find(\"span\", string=True, recursive=False)\n",
    "                        if span:\n",
    "                            if \"Loại hình bất động sản\" in label.text:\n",
    "                                property_type = span.text.strip()\n",
    "                            elif \"Diện tích đất\" in label.text:\n",
    "                                area = span.text.strip()\n",
    "                            elif \"Diện tích sử dụng\" in label.text:\n",
    "                                area = span.text.strip()\n",
    "                            elif \"Giấy tờ pháp lý\" in label.text:\n",
    "                                legal_documents = span.text.strip()\n",
    "                            elif \"Tổng số tầng\" in label.text:\n",
    "                                floors = span.text.strip()\n",
    "                            elif \"Số phòng ngủ\" in label.text:\n",
    "                                bedrooms = span.text.strip()\n",
    "                            elif \"Số phòng vệ sinh\" in label.text:\n",
    "                                toilets = span.text.strip()\n",
    "                            elif \"Hướng cửa chính\" in label.text:\n",
    "                                direction = span.text.strip()\n",
    "\n",
    "             # Trích xuất thông tin \"Chi tiết\"\n",
    "            detail_div = soup.find(\"div\", class_=\"sc-6orc5o-18 gdAVnx\") \n",
    "\n",
    "            for child in detail_div.find_all(\"div\", class_=\"phone-wrapper\"):\n",
    "                child.decompose()\n",
    "                \n",
    "            for tag in detail_div.find_all(True):  # Tìm tất cả các thẻ con\n",
    "                if tag.name != 'br':  # Nếu tên thẻ không phải là <br>\n",
    "                    tag.unwrap()  # Giữ lại nội dung của thẻ và loại bỏ thẻ đó\n",
    "\n",
    "            details = detail_div.decode_contents()\n",
    "            \n",
    "            phone_wrapper = soup.find(\"div\", class_=\"phone-wrapper\")\n",
    "            if phone_wrapper:\n",
    "                phone_span = phone_wrapper.find(\"span\", class_=\"phone-hidden\")\n",
    "                if phone_span and 'data-phone' in phone_span.attrs:\n",
    "                    phone_number = phone_span['data-phone']\n",
    "            \n",
    "            # Trích xuất link ảnh\n",
    "            image_div = soup.select_one(\"div.slick-slide.slick-active.slick-current img\")\n",
    "            image_link = None\n",
    "            if image_div and 'src' in image_div.attrs:\n",
    "                image_link = image_div['src']\n",
    "\n",
    "            # Kiểm tra nếu không có link ảnh thì bỏ qua\n",
    "            if image_link is None or area is None:\n",
    "                return None\n",
    "\n",
    "            # Tạo một từ điển chứa thông tin từng bài đăng\n",
    "            item = {\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"area\": area,\n",
    "                \"address\": address,\n",
    "                \"description\": details,\n",
    "                \"type\": \"Cho Thuê\",\n",
    "                \"type_estate\": property_type,\n",
    "                \"law\": legal_documents,\n",
    "                \"floor\": floors,\n",
    "                \"bedroom\": bedrooms,\n",
    "                \"toilet\": toilets,\n",
    "                \"direction\": direction,\n",
    "                \"image_links\": image_link,\n",
    "                \"phonenumber\": phone_number\n",
    "            }                \n",
    "            return item        \n",
    "        else:\n",
    "            print(f\"Lỗi khi truy cập vào URL: {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ URL: {url}, Lỗi: {e}\")\n",
    "        return None\n",
    "\n",
    "# Tạo một danh sách để lưu trữ thông tin từ mỗi URL\n",
    "data_list = []\n",
    "\n",
    "# Đọc nội dung từ tệp rent_urls.txt\n",
    "with open(\"rent_urls.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    urls = file.readlines()\n",
    "\n",
    "# Lặp qua từng URL để lấy thông tin\n",
    "for url in urls:\n",
    "    # Xóa khoảng trắng và ký tự xuống dòng từ URL\n",
    "    url = url.strip()\n",
    "    # Trích xuất thông tin từ URL và thêm vào danh sách\n",
    "    item = extract_info_from_url(url)\n",
    "    if item:\n",
    "        data_list.append(item)\n",
    "\n",
    "# Lưu danh sách dưới dạng JSON vào tệp rent_data.json\n",
    "with open(\"rent_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dữ liệu đã được lưu vào tệp rent_data.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
